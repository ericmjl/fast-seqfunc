{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fast-SeqFunc Documentation","text":"<p><code>fast-seqfunc</code> is a Python library for building sequence-function models quickly and easily, leveraging PyCaret and machine learning techniques to predict functional properties from biological sequences.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quickstart Tutorial - Learn the basics of training and using sequence-function models</li> <li>Regression Tutorial - Learn how to predict continuous values from sequences</li> <li>Classification Tutorial - Learn how to classify sequences into discrete categories</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install <code>fast-seqfunc</code> using pip:</p> <pre><code>pip install fast-seqfunc\n</code></pre> <p>Or directly from GitHub for the latest version:</p> <pre><code>pip install git+https://github.com/ericmjl/fast-seqfunc.git\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy-to-use API: Train models and make predictions with just a few lines of code</li> <li>Automatic Model Selection: Uses PyCaret to automatically compare and select the best model</li> <li>Sequence Embedding: Currently supports one-hot encoding with more methods coming soon</li> <li>Regression and Classification: Support for both continuous values and categorical outputs</li> <li>Comprehensive Evaluation: Built-in metrics and visualization utilities</li> </ul>"},{"location":"#why-fast-seqfunc","title":"Why Fast-SeqFunc?","text":"<p>The primary motivation behind Fast-SeqFunc is to quickly answer a crucial question in sequence-function modeling: Is there detectable signal in my data?</p> <p>In biological sequence-function problems, determining whether a predictive relationship exists is a critical first step before investing significant resources in complex modeling approaches. Fast-SeqFunc allows you to:</p> <ol> <li>Rapidly detect signal: Quickly build baseline models to determine if your sequence data contains predictive information</li> <li>Make early decisions: Identify promising directions early in your research process</li> <li>Rank candidates efficiently: Use simple but effective models to score and prioritize candidate sequences for experimental testing</li> <li>Validate before scaling: Confirm signal exists before investing time in developing more complex neural network models</li> <li>Iterate strategically: When signal is detected, use that knowledge to guide the development of more sophisticated models</li> </ol> <p>By providing a fast path to baseline model development, Fast-SeqFunc helps you make informed decisions about where to focus your modeling efforts.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":""},{"location":"#command-line-interface","title":"Command-Line Interface","text":"<p>Fast-SeqFunc provides a convenient command-line interface for common tasks:</p> <pre><code># Train a model\nfast-seqfunc train train_data.csv --sequence-col sequence --target-col function --embedding-method one-hot\n\n# Make predictions with a trained model\nfast-seqfunc predict-cmd model.pkl new_sequences.csv --sequence-col sequence --output-dir predictions --predictions-filename predictions.csv\n\n# Compare different embedding methods\nfast-seqfunc compare-embeddings train_data.csv --test-data test_data.csv\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<p>You can also use Fast-SeqFunc programmatically in your Python code:</p> <pre><code>from fast_seqfunc import train_model, predict, save_model\n\n# Train a model\nmodel_info = train_model(\n    train_data=train_df,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"regression\"\n)\n\n# Make predictions\npredictions = predict(model_info, new_sequences)\n\n# Save the model\nsave_model(model_info, \"model.pkl\")\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<p>Future development plans include:</p> <ol> <li>Additional embedding methods (ESM, CARP, etc.)</li> <li>Integration with more advanced deep learning models</li> <li>Enhanced visualization and interpretation tools</li> <li>Expanded support for various sequence types</li> <li>Benchmarking against established methods</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request or open an issue to discuss improvements or feature requests.</p>"},{"location":"api/","title":"Top-level API for fast-seqfunc","text":"<p>::: fast_seqfunc</p>"},{"location":"api/#api-reference","title":"API Reference","text":"<p>This page provides the API reference for Fast-SeqFunc.</p>"},{"location":"api/#core-api","title":"Core API","text":"<p>These are the main functions you'll use to train models and make predictions.</p> <p>::: fast_seqfunc.core     options:       show_root_heading: false       show_source: false</p>"},{"location":"api/#embedders","title":"Embedders","text":"<p>Sequence embedding methods to convert protein or nucleotide sequences into numerical representations.</p> <p>::: fast_seqfunc.embedders     options:       show_root_heading: false       show_source: false</p>"},{"location":"api/#models","title":"Models","text":"<p>Model classes for sequence-function prediction.</p> <p>::: fast_seqfunc.models     options:       show_root_heading: false       show_source: false</p>"},{"location":"api/#cli","title":"CLI","text":"<p>Command-line interface for Fast-SeqFunc.</p> <p>::: fast_seqfunc.cli     options:       show_root_heading: false       show_source: false</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This document provides details on the main functions and classes available in the <code>fast-seqfunc</code> package.</p>"},{"location":"api_reference/#core-functions","title":"Core Functions","text":""},{"location":"api_reference/#train_model","title":"<code>train_model</code>","text":"<pre><code>from fast_seqfunc import train_model\n\nmodel_info = train_model(\n    train_data,\n    val_data=None,\n    test_data=None,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"regression\",\n    optimization_metric=None,\n    **kwargs\n)\n</code></pre> <p>Trains a sequence-function model using PyCaret.</p> <p>Parameters:</p> <ul> <li><code>train_data</code>: DataFrame or path to CSV file with training data.</li> <li><code>val_data</code>: Optional validation data (not directly used, reserved for future).</li> <li><code>test_data</code>: Optional test data for final evaluation.</li> <li><code>sequence_col</code>: Column name containing sequences.</li> <li><code>target_col</code>: Column name containing target values.</li> <li><code>embedding_method</code>: Method to use for embedding sequences. Currently only \"one-hot\" is supported.</li> <li><code>model_type</code>: Type of modeling problem (\"regression\" or \"classification\").</li> <li><code>optimization_metric</code>: Metric to optimize during model selection (e.g., \"r2\", \"accuracy\", \"f1\").</li> <li><code>**kwargs</code>: Additional arguments passed to PyCaret setup.</li> </ul> <p>Returns:</p> <ul> <li>Dictionary containing the trained model and related metadata.</li> </ul>"},{"location":"api_reference/#predict","title":"<code>predict</code>","text":"<pre><code>from fast_seqfunc import predict\n\npredictions = predict(\n    model_info,\n    sequences,\n    sequence_col=\"sequence\"\n)\n</code></pre> <p>Generates predictions for new sequences using a trained model.</p> <p>Parameters:</p> <ul> <li><code>model_info</code>: Dictionary from <code>train_model</code> containing model and related information.</li> <li><code>sequences</code>: Sequences to predict (list, Series, or DataFrame).</li> <li><code>sequence_col</code>: Column name in DataFrame containing sequences.</li> </ul> <p>Returns:</p> <ul> <li>Array of predictions.</li> </ul>"},{"location":"api_reference/#save_model","title":"<code>save_model</code>","text":"<pre><code>from fast_seqfunc import save_model\n\nsave_model(model_info, path)\n</code></pre> <p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li><code>model_info</code>: Dictionary containing model and related information.</li> <li><code>path</code>: Path to save the model.</li> </ul> <p>Returns:</p> <ul> <li>None</li> </ul>"},{"location":"api_reference/#load_model","title":"<code>load_model</code>","text":"<pre><code>from fast_seqfunc import load_model\n\nmodel_info = load_model(path)\n</code></pre> <p>Loads a trained model from disk.</p> <p>Parameters:</p> <ul> <li><code>path</code>: Path to saved model file.</li> </ul> <p>Returns:</p> <ul> <li>Dictionary containing the model and related information.</li> </ul>"},{"location":"api_reference/#embedder-classes","title":"Embedder Classes","text":""},{"location":"api_reference/#onehotembedder","title":"<code>OneHotEmbedder</code>","text":"<pre><code>from fast_seqfunc.embedders import OneHotEmbedder\n\nembedder = OneHotEmbedder(sequence_type=\"auto\")\nembeddings = embedder.fit_transform(sequences)\n</code></pre> <p>One-hot encoding for protein or nucleotide sequences.</p> <p>Parameters:</p> <ul> <li><code>sequence_type</code>: Type of sequences to encode (\"protein\", \"dna\", \"rna\", or \"auto\").</li> </ul> <p>Methods:</p> <ul> <li><code>fit(sequences)</code>: Determine alphabet and set up the embedder.</li> <li><code>transform(sequences)</code>: Transform sequences to one-hot encodings.</li> <li><code>fit_transform(sequences)</code>: Fit and transform in one step.</li> </ul>"},{"location":"api_reference/#helper-functions","title":"Helper Functions","text":""},{"location":"api_reference/#get_embedder","title":"<code>get_embedder</code>","text":"<pre><code>from fast_seqfunc.embedders import get_embedder\n\nembedder = get_embedder(method=\"one-hot\")\n</code></pre> <p>Get an embedder instance based on method name.</p> <p>Parameters:</p> <ul> <li><code>method</code>: Embedding method (currently only \"one-hot\" is supported).</li> </ul> <p>Returns:</p> <ul> <li>Configured embedder instance.</li> </ul>"},{"location":"api_reference/#evaluate_model","title":"<code>evaluate_model</code>","text":"<pre><code>from fast_seqfunc.core import evaluate_model\n\nresults = evaluate_model(\n    model,\n    X_test,\n    y_test,\n    embedder,\n    model_type,\n    embed_cols\n)\n</code></pre> <p>Evaluate model performance on test data.</p> <p>Parameters:</p> <ul> <li><code>model</code>: Trained model.</li> <li><code>X_test</code>: Test sequences.</li> <li><code>y_test</code>: True target values.</li> <li><code>embedder</code>: Embedder to transform sequences.</li> <li><code>model_type</code>: Type of model (regression or classification).</li> <li><code>embed_cols</code>: Column names for embedded features.</li> </ul> <p>Returns:</p> <ul> <li>Dictionary containing metrics and prediction data with structure:   <pre><code>{\n   \"metrics\": {metric_name: value, ...},\n   \"predictions_data\": {\n       \"y_true\": [...],\n       \"y_pred\": [...]\n   }\n}\n</code></pre></li> </ul>"},{"location":"api_reference/#save_detailed_metrics","title":"<code>save_detailed_metrics</code>","text":"<pre><code>from fast_seqfunc.core import save_detailed_metrics\n\nsave_detailed_metrics(\n    metrics_data,\n    output_dir,\n    model_type,\n    embedding_method=\"unknown\"\n)\n</code></pre> <p>Save detailed model metrics to files in the specified directory.</p> <p>Parameters:</p> <ul> <li><code>metrics_data</code>: Dictionary containing metrics and prediction data from <code>evaluate_model</code>.</li> <li><code>output_dir</code>: Directory to save metrics files.</li> <li><code>model_type</code>: Type of model (regression or classification).</li> <li><code>embedding_method</code>: Embedding method used for this model.</li> </ul> <p>Returns:</p> <ul> <li>None</li> </ul> <p>Output Files:</p> <ul> <li>JSON file with detailed metrics</li> <li>CSV file with raw predictions and true values</li> <li>Visualization plots based on model type:</li> <li>For regression: scatter plot, residual plot</li> <li>For classification: confusion matrix</li> </ul>"},{"location":"quickstart/","title":"Fast-SeqFunc Quickstart","text":"<p>This guide demonstrates how to use <code>fast-seqfunc</code> for training sequence-function models and making predictions with your own sequence data.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>The <code>fast-seqfunc</code> package installed</li> </ul>"},{"location":"quickstart/#setup","title":"Setup","text":"<p>The <code>fast-seqfunc</code> package comes with a command-line interface (CLI) that makes it easy to train models and make predictions without writing any code.</p> <p>To see all available commands, run:</p> <pre><code>fast-seqfunc --help\n</code></pre> <p>For help with a specific command, use:</p> <pre><code>fast-seqfunc [command] --help\n</code></pre>"},{"location":"quickstart/#data-preparation","title":"Data Preparation","text":"<p>For this tutorial, we assume you already have a sequence-function dataset with the following format:</p> <pre><code>sequence,function\nACGTACGT...,0.75\nTACGTACG...,0.63\n...\n</code></pre> <p>You'll need to split your data into training and test sets. You can use any CSV file manipulation tool for this, or you can use the built-in synthetic data generator to create sample data:</p> <pre><code># Generate synthetic regression data (DNA sequences with G count function)\nfast-seqfunc generate-synthetic g_count --output-dir data --total-count 1000 --split-data\n</code></pre> <p>This will create <code>train.csv</code>, <code>val.csv</code>, and <code>test.csv</code> files in the <code>data</code> directory.</p>"},{"location":"quickstart/#training-a-model","title":"Training a Model","text":"<p>With <code>fast-seqfunc</code>, you can train a model with a single command:</p> <pre><code># Train and compare multiple models automatically\nfast-seqfunc train data/train.csv \\\n  --val-data data/val.csv \\\n  --test-data data/test.csv \\\n  --sequence-col sequence \\\n  --target-col function \\\n  --embedding-method one-hot \\\n  --model-type regression \\\n  --output-dir outputs\n\n# The model will be saved to outputs/model.pkl by default\n</code></pre> <p>The command above will:</p> <ol> <li>Load your training, validation, and test data</li> <li>Embed the sequences using one-hot encoding</li> <li>Train multiple regression models using PyCaret</li> <li>Select the best model based on performance</li> <li>Evaluate the model on the test data</li> <li>Save the model and performance metrics</li> </ol>"},{"location":"quickstart/#making-predictions","title":"Making Predictions","text":"<p>Making predictions on new sequences is straightforward:</p> <pre><code># Make predictions on test data\nfast-seqfunc predict-cmd outputs/model.pkl data/test.csv \\\n  --sequence-col sequence \\\n  --output-dir prediction_outputs \\\n  --predictions-filename predictions.csv\n\n# Results will be saved to prediction_outputs/predictions.csv\n# A histogram of predictions will be generated (if applicable)\n</code></pre> <p>This command will:</p> <ol> <li>Load your trained model</li> <li>Load the sequences from your test data</li> <li>Generate predictions for each sequence</li> <li>Save the results to a CSV file with both the original sequences and the predictions</li> </ol>"},{"location":"quickstart/#comparing-embedding-methods","title":"Comparing Embedding Methods","text":"<p>You can also compare different embedding methods to see which works best for your data:</p> <pre><code># Compare different embedding methods on the same dataset\nfast-seqfunc compare-embeddings data/train.csv \\\n  --test-data data/test.csv \\\n  --sequence-col sequence \\\n  --target-col function \\\n  --model-type regression \\\n  --output-dir comparison_outputs\n\n# Results will be saved to comparison_outputs/embedding_comparison.csv\n# Individual models will be saved in comparison_outputs/models/\n</code></pre> <p>This command will:</p> <ol> <li>Train models using different embedding methods (one-hot, and others if available)</li> <li>Evaluate each model on the test data</li> <li>Compare the performance metrics</li> <li>Save the results and models</li> </ol>"},{"location":"quickstart/#generating-synthetic-data","title":"Generating Synthetic Data","text":"<p>Fast-SeqFunc includes a powerful synthetic data generator for different sequence-function relationships:</p> <pre><code># See available synthetic data tasks\nfast-seqfunc list-synthetic-tasks\n\n# Generate data for a specific task\nfast-seqfunc generate-synthetic motif_position \\\n  --sequence-type dna \\\n  --motif ATCG \\\n  --noise-level 0.2 \\\n  --output-dir data/motif_task\n\n# Generate classification data\nfast-seqfunc generate-synthetic classification \\\n  --sequence-type protein \\\n  --output-dir data/classification_task\n</code></pre> <p>The synthetic data generator can create datasets with various sequence-function relationships, including:</p> <ul> <li>Linear relationships (G count, GC content)</li> <li>Position-dependent functions (motif position)</li> <li>Nonlinear relationships (length-dependent functions)</li> <li>Classification problems (presence/absence of patterns)</li> <li>And many more!</li> </ul>"},{"location":"quickstart/#interpreting-results-for-signal-detection","title":"Interpreting Results for Signal Detection","text":"<p>One of the primary purposes of Fast-SeqFunc is to quickly determine if there is meaningful \"signal\" in your sequence-function data. Here's how to interpret your results:</p>"},{"location":"quickstart/#evaluating-signal-presence","title":"Evaluating Signal Presence","text":"<ol> <li>Check performance metrics:</li> <li>For regression: R\u00b2, RMSE, and MAE values</li> <li> <p>For classification: Accuracy, F1 score, AUC-ROC</p> </li> <li> <p>Use visualizations:</p> </li> <li>Scatter plots of predicted vs. actual values</li> <li>Residual plots showing systematic patterns or random noise</li> <li> <p>ROC curves for classification tasks</p> </li> <li> <p>Benchmarks for determining signal:</p> </li> <li>Models significantly outperforming random guessing indicate signal</li> <li>R\u00b2 values above 0.3-0.4 suggest detectable relationships</li> <li>AUC-ROC values above 0.6-0.7 indicate useful classification signal</li> </ol>"},{"location":"quickstart/#leveraging-early-signal","title":"Leveraging Early Signal","text":"<p>When you detect signal:</p> <ol> <li>Prioritize candidates: Use model predictions to rank and select promising sequences for experimental testing</li> <li>Iterate experimentally: Test top-ranked sequences and use results to refine your model</li> <li>Decide on complexity: Strong signal warrants investment in more sophisticated models like neural networks</li> <li>Compare embedding methods: If signal is present, explore if more complex embeddings (ESM, CARP) improve performance</li> </ol> <p>Remember that even modest performance can be valuable for prioritizing experimental candidates and guiding exploration of sequence space.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>After mastering the basics, you can:</p> <ol> <li>Try different embedding methods (currently only <code>one-hot</code> is supported, with more coming soon)</li> <li>Experiment with classification problems by setting <code>--model-type classification</code></li> <li>Generate different types of synthetic data to benchmark your approach</li> <li>Explore the Python API for more advanced customization</li> </ol> <p>For more details, check out the API documentation.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>This document outlines the planned development path for fast-seqfunc.</p>"},{"location":"roadmap/#current-roadmap-items","title":"Current Roadmap Items","text":""},{"location":"roadmap/#additional-predictor-columns","title":"Additional Predictor Columns","text":"<p>Extend Fast-SeqFunc to incorporate additional predictor columns alongside sequence data, enabling context-aware sequence-function modeling. This will allow users to include experimental conditions, environmental factors, or other relevant variables that may influence sequence function relationships.</p> <ol> <li>Preprocessing Pipeline: Automatic handling of numerical and categorical additional predictors</li> <li>Enhanced CLI Support: Command-line options for specifying additional predictor columns</li> <li>Serialization Enhancements: Extended model format to include additional predictor information</li> <li>Feature Importance Analysis: Tools to understand the relative importance of sequence features vs. additional predictors</li> </ol> <p>See the detailed design document in <code>docs/design/additional_predictors.md</code>.</p>"},{"location":"roadmap/#expanded-embedding-methods","title":"Expanded Embedding Methods","text":"<p>Support for more sequence embedding methods beyond one-hot encoding. While the CLI and core functions reference \"carp\" and \"esm2\" embedders, these are not currently implemented. Integrating with ESM2, CARP, or other pre-trained models will enhance the library's capabilities.</p>"},{"location":"roadmap/#signal-detection-and-analysis-tools","title":"Signal Detection and Analysis Tools","text":"<p>Enhance Fast-SeqFunc's primary purpose of detecting signal in sequence-function data:</p> <ol> <li>Automated Signal Detection Reports: Generate comprehensive reports that clearly indicate whether signal exists in the data with statistical confidence measures</li> <li>Benchmark Comparisons: Automatically compare model performance against random expectation and theoretical upper bounds</li> <li>Advanced Visualization Tools: Create specialized visualizations that highlight the presence or absence of signal in different ways</li> <li>Candidate Generation Strategies: Develop smarter approaches for generating candidate sequences when signal is detected</li> <li>Signal-Guided Model Selection: Automatically recommend more advanced modeling approaches when sufficient signal is detected</li> <li>Effect Size Analysis: Tools to estimate the magnitude of sequence effects on function</li> </ol>"},{"location":"roadmap/#batch-processing-for-large-datasets","title":"Batch Processing for Large Datasets","text":"<p>Implement efficient batch processing for datasets that are too large to fit in memory, especially when using more complex embedding methods that require significant computational resources.</p>"},{"location":"roadmap/#cluster-based-cross-validation-framework","title":"Cluster-Based Cross-Validation Framework","text":"<p>Enhance the validation strategy with cluster-based cross-validation, where sequences are clustered at a specified identity level (e.g., using CD-HIT) and entire clusters are left out during training. This approach provides a more realistic assessment of model generalizability to truly novel sequences.</p>"},{"location":"roadmap/#onnx-model-integration","title":"ONNX Model Integration","text":"<p>Add support for exporting models to ONNX format and rehydrating models from ONNX rather than pickle files, improving portability, performance, and security.</p>"},{"location":"roadmap/#caching-mechanism","title":"Caching Mechanism","text":"<p>Add disk caching for embeddings to improve performance on repeated runs with the same sequences.</p>"},{"location":"roadmap/#enhanced-visualization-options","title":"Enhanced Visualization Options","text":"<p>Develop built-in visualizations for model performance and sequence importance analysis to help users better understand their models.</p>"},{"location":"roadmap/#fasta-file-support","title":"FASTA File Support","text":"<p>Add direct support for loading sequence data from FASTA files, a common format in bioinformatics.</p>"},{"location":"roadmap/#completed-items","title":"Completed Items","text":"<p>The following items from the previous roadmap have been implemented and are now available:</p> <ul> <li> <p>Custom Alphabets via Configuration File: The <code>Alphabet</code> class supports loading/saving custom alphabets through JSON configuration files with the <code>from_json</code> and <code>to_json</code> methods.</p> </li> <li> <p>Auto-Inferred Alphabets: The <code>infer_alphabet</code> function can automatically infer alphabets from input sequences, and is exposed in the public API.</p> </li> <li> <p>Automatic Cluster Splits: Basic data splitting capabilities are available through the synthetic data generation functionality, though not based on sequence clustering.</p> </li> </ul>"},{"location":"roadmap/#future-considerations","title":"Future Considerations","text":"<p>Additional roadmap items will be added here after review.</p>"},{"location":"design/additional_predictors/","title":"Additional Predictors in Fast-SeqFunc: Design Document","text":""},{"location":"design/additional_predictors/#overview","title":"Overview","text":"<p>Fast-SeqFunc is designed to model the relationship between biological sequences and a target function/property. Currently, it supports using only the sequence as the predictor. This design document outlines the necessary changes to expand Fast-SeqFunc to incorporate additional predictor columns alongside the sequence data.</p>"},{"location":"design/additional_predictors/#motivation","title":"Motivation","text":"<p>In many real-world applications, sequence-function relationships may depend on additional contextual variables or experimental conditions. For example:</p> <ol> <li>Protein function might depend not only on the amino acid sequence but also on pH, temperature, or salt concentration</li> <li>Gene expression levels may depend on the DNA sequence as well as cell type, developmental stage, or treatment conditions</li> <li>Binding affinity might depend on sequence and additional information about binding partners</li> </ol> <p>By supporting additional predictor columns, Fast-SeqFunc will become more versatile and applicable to a wider range of biological problems where context matters.</p>"},{"location":"design/additional_predictors/#design-goals","title":"Design Goals","text":"<ol> <li>Maintain API Simplicity: Preserve the current simple API while extending it to handle additional predictors</li> <li>Backward Compatibility: Ensure existing code continues to work without changes</li> <li>Flexible Integration: Allow for simple integration of sequence embeddings with additional predictors</li> <li>CLI Support: Extend the command-line interface to handle additional predictor columns</li> <li>Consistent Implementation: Apply changes consistently across both the Python API and CLI</li> </ol>"},{"location":"design/additional_predictors/#architecture-changes","title":"Architecture Changes","text":""},{"location":"design/additional_predictors/#core-components-affected","title":"Core Components Affected","text":"<ol> <li>Core API (<code>core.py</code>):</li> <li>Extend <code>train_model</code> function to accept additional predictor columns</li> <li> <p>Modify data processing to incorporate additional predictors with embeddings</p> </li> <li> <p>Models (<code>models.py</code>):</p> </li> <li> <p>Update <code>SequenceFunctionModel</code> to handle additional predictors alongside sequence embeddings</p> </li> <li> <p>CLI (<code>cli.py</code>):</p> </li> <li>Add options to specify additional predictor columns</li> <li> <p>Update data processing for CLI commands</p> </li> <li> <p>Documentation:</p> </li> <li>Update API reference</li> <li>Add examples showing how to use additional predictors</li> </ol>"},{"location":"design/additional_predictors/#data-flow-modifications","title":"Data Flow Modifications","text":"<p>Current data flow:</p> <ol> <li>User provides sequences + target values</li> <li>Sequences are embedded</li> <li>ML models are trained on embeddings</li> </ol> <p>New data flow:</p> <ol> <li>User provides sequences + additional predictors + target values</li> <li>Sequences are embedded</li> <li>Embeddings are combined with additional predictors</li> <li>ML models are trained on the combined features</li> </ol>"},{"location":"design/additional_predictors/#api-design","title":"API Design","text":""},{"location":"design/additional_predictors/#python-api-changes","title":"Python API Changes","text":""},{"location":"design/additional_predictors/#current-api","title":"Current API","text":"<pre><code>model_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"regression\",\n    optimization_metric=\"r2\",\n)\n</code></pre>"},{"location":"design/additional_predictors/#enhanced-api","title":"Enhanced API","text":"<pre><code>model_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    additional_predictor_cols=[\"pH\", \"temperature\"],  # New parameter\n    embedding_method=\"one-hot\",\n    model_type=\"regression\",\n    optimization_metric=\"r2\",\n)\n</code></pre>"},{"location":"design/additional_predictors/#predict-function-changes","title":"Predict Function Changes","text":"<p>Current:</p> <pre><code>predictions = predict(model_info, new_sequences)\n</code></pre> <p>Enhanced:</p> <pre><code>predictions = predict(\n    model_info,\n    new_data,  # Now can be DataFrame with sequence and additional predictor columns\n    sequence_col=\"sequence\"\n)\n</code></pre>"},{"location":"design/additional_predictors/#cli-changes","title":"CLI Changes","text":"<p>Current CLI:</p> <pre><code>fast-seqfunc train train_data.csv --sequence-col sequence --target-col function\n</code></pre> <p>Enhanced CLI:</p> <pre><code>fast-seqfunc train train_data.csv --sequence-col sequence --target-col function --additional-predictors pH,temperature\n</code></pre>"},{"location":"design/additional_predictors/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"design/additional_predictors/#feature-combination-method","title":"Feature Combination Method","text":"<p>The implementation will use a simple concatenation approach to combine sequence embeddings with additional predictors. This means that additional predictor columns will be appended to the sequence embedding features to create the final feature matrix for model training.</p>"},{"location":"design/additional_predictors/#data-processing-flow","title":"Data Processing Flow","text":"<ol> <li>Load CSV or DataFrame data as currently implemented</li> <li>Validate presence of additional predictor columns if specified</li> <li>Embed sequence data using the existing embedding pipeline</li> <li>Process additional predictors (scaling, handling missing values, etc.)</li> <li>Combine sequence embeddings with additional predictors</li> <li>Train models on the combined feature set</li> </ol>"},{"location":"design/additional_predictors/#data-validation-and-preprocessing","title":"Data Validation and Preprocessing","text":"<p>Additional predictors may require preprocessing:</p> <ul> <li>Handling missing values</li> <li>Scaling numerical features</li> <li>Encoding categorical features</li> <li>Type validation</li> </ul> <p>We'll implement a preprocessing pipeline for additional predictors that handles these tasks automatically.</p>"},{"location":"design/additional_predictors/#model-information-enhancement","title":"Model Information Enhancement","text":"<p>The model_info dictionary will be enhanced to include:</p> <pre><code>{\n    \"model\": trained_model,\n    \"model_type\": model_type,\n    \"embedder\": embedder,\n    \"embed_cols\": embed_cols,\n    \"additional_predictor_cols\": additional_predictor_cols,  # New\n    \"additional_predictor_preprocessing\": preprocessing_pipeline,  # New\n    \"test_results\": test_results,\n}\n</code></pre> <p>This ensures all information needed for making predictions with additional predictors is preserved.</p>"},{"location":"design/additional_predictors/#serialization-changes","title":"Serialization Changes","text":"<p>The model serialization format will need to include information about additional predictors. We'll maintain backward compatibility by checking for the presence of additional predictor information when loading existing models.</p>"},{"location":"design/additional_predictors/#code-changes-required","title":"Code Changes Required","text":""},{"location":"design/additional_predictors/#in-corepy","title":"In <code>core.py</code>","text":"<ol> <li>Update <code>train_model</code> function signature to accept additional predictors</li> <li>Modify data processing to handle additional predictors</li> <li>Update model information dictionary to include additional predictor metadata</li> <li>Modify <code>predict</code> function to handle additional predictors in prediction data</li> </ol>"},{"location":"design/additional_predictors/#in-modelspy","title":"In <code>models.py</code>","text":"<ol> <li>Update <code>SequenceFunctionModel</code> class to handle additional predictors</li> <li>Modify the <code>fit</code> and <code>predict</code> methods to incorporate additional predictors</li> <li>Update serialization methods to handle additional predictor information</li> </ol>"},{"location":"design/additional_predictors/#in-clipy","title":"In <code>cli.py</code>","text":"<ol> <li>Add new CLI options for specifying additional predictors</li> <li>Update data loading and processing in CLI commands</li> <li>Update help text and documentation</li> </ol>"},{"location":"design/additional_predictors/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>To maintain backward compatibility:</p> <ul> <li>All new parameters will be optional with sensible defaults</li> <li>Existing code paths will work without modification</li> <li>Model serialization will be backward compatible</li> </ul>"},{"location":"design/additional_predictors/#testing-strategy","title":"Testing Strategy","text":"<p>Tests will be expanded to cover:</p> <ol> <li>Training with various combinations of additional predictors</li> <li>Making predictions with additional predictors</li> <li>Serialization and deserialization of models with additional predictor information</li> <li>CLI functionality for additional predictors</li> <li>Edge cases (missing values, type mismatches, etc.)</li> </ol>"},{"location":"design/additional_predictors/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Add automated feature selection for additional predictors</li> <li>Support for feature importance analysis that includes additional predictors</li> <li>Add specialized visualizations for understanding the impact of additional predictors</li> </ol>"},{"location":"design/additional_predictors/#conclusion","title":"Conclusion","text":"<p>Adding support for additional predictor columns will significantly enhance the versatility of Fast-SeqFunc, making it applicable to a wider range of biological problems where context matters alongside sequence. The implementation will maintain the simplicity and user-friendliness of the current API while providing powerful new capabilities.</p>"},{"location":"design/additional_predictors_example/","title":"Additional Predictors: Usage Examples","text":"<p>This document provides examples of how to use additional predictor columns with Fast-SeqFunc.</p>"},{"location":"design/additional_predictors_example/#example-1-basic-usage-with-python-api","title":"Example 1: Basic Usage with Python API","text":"<p>The following example demonstrates how to train a model with protein sequences and additional predictors (pH and temperature) that may affect protein function:</p> <pre><code>import pandas as pd\nfrom fast_seqfunc import train_model, predict\n\n# Sample data with sequences, additional predictors, and function values\ndata = pd.DataFrame({\n    'sequence': ['MKALIVLGL', 'MKHPIVLLL', 'MKLIVPMGL', 'MKAIVLELL'],\n    'pH': [6.5, 7.0, 7.5, 8.0],\n    'temperature': [25, 30, 35, 40],\n    'activity': [0.45, 0.62, 0.78, 0.34]\n})\n\n# Split into train and test sets\ntrain_data = data.iloc[:3]\ntest_data = data.iloc[3:]\n\n# Train model using sequence and additional predictors\nmodel_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col='sequence',\n    target_col='activity',\n    additional_predictor_cols=['pH', 'temperature'],\n    embedding_method='one-hot',\n    model_type='regression',\n    optimization_metric='r2'\n)\n\n# Make predictions on new data\nnew_data = pd.DataFrame({\n    'sequence': ['MKAIVLELL', 'MKLIVLELL'],\n    'pH': [7.0, 7.5],\n    'temperature': [37, 38]\n})\n\npredictions = predict(model_info, new_data)\nprint(f\"Predicted activities: {predictions}\")\n</code></pre>"},{"location":"design/additional_predictors_example/#example-2-using-the-cli","title":"Example 2: Using the CLI","text":"<p>You can also use additional predictors via the command-line interface:</p> <pre><code># Train a model with additional predictors\nfast-seqfunc train protein_data.csv \\\n    --sequence-col sequence \\\n    --target-col activity \\\n    --additional-predictors pH,temperature \\\n    --embedding-method one-hot \\\n    --model-type regression\n\n# Make predictions\nfast-seqfunc predict model.pkl new_sequences.csv \\\n    --output-path predictions.csv\n</code></pre> <p>The input CSV files should contain the sequence column, the target column, and any additional predictor columns specified.</p>"},{"location":"design/additional_predictors_example/#example-3-handling-categorical-predictors","title":"Example 3: Handling Categorical Predictors","text":"<p>Additional predictors can be numeric or categorical. Fast-SeqFunc will automatically handle the encoding of categorical predictors:</p> <pre><code>import pandas as pd\nfrom fast_seqfunc import train_model\n\n# Data with both numeric and categorical predictors\ndata = pd.DataFrame({\n    'sequence': ['MKALIVLGL', 'MKHPIVLLL', 'MKLIVPMGL', 'MKAIVLELL'],\n    'pH': [6.5, 7.0, 7.5, 8.0],\n    'buffer_type': ['phosphate', 'tris', 'phosphate', 'tris'],\n    'cell_line': ['HEK293', 'CHO', 'HEK293', 'CHO'],\n    'activity': [0.45, 0.62, 0.78, 0.34]\n})\n\n# Train model with both numeric and categorical predictors\nmodel_info = train_model(\n    train_data=data,\n    sequence_col='sequence',\n    target_col='activity',\n    additional_predictor_cols=['pH', 'buffer_type', 'cell_line'],\n    embedding_method='one-hot',\n    model_type='regression'\n)\n</code></pre>"},{"location":"design/additional_predictors_example/#example-4-analyzing-feature-importance","title":"Example 4: Analyzing Feature Importance","text":"<p>With additional predictors, it becomes important to understand their relative importance:</p> <pre><code>import matplotlib.pyplot as plt\nfrom fast_seqfunc import train_model, feature_importance\n\n# Train model with additional predictors\nmodel_info = train_model(\n    train_data=data,\n    sequence_col='sequence',\n    target_col='activity',\n    additional_predictor_cols=['pH', 'temperature', 'buffer_type']\n)\n\n# Get feature importance\nimportance = feature_importance(model_info)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nimportance.plot(kind='bar')\nplt.title('Feature Importance')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"design/additional_predictors_example/#example-5-saving-and-loading-models","title":"Example 5: Saving and Loading Models","text":"<p>Models with additional predictors can be saved and loaded just like regular models:</p> <pre><code>from fast_seqfunc import save_model, load_model\n\n# Save model\nsave_model(model_info, 'protein_activity_model.pkl')\n\n# Load model\nloaded_model = load_model('protein_activity_model.pkl')\n\n# Make predictions using the loaded model\npredictions = predict(\n    loaded_model,\n    new_data,\n    sequence_col='sequence'\n)\n</code></pre>"},{"location":"design/custom_alphabets/","title":"Custom Alphabets Design Document","text":""},{"location":"design/custom_alphabets/#overview","title":"Overview","text":"<p>This document outlines the design for enhancing fast-seqfunc with support for custom alphabets, particularly focusing on handling mixed-length characters and various sequence storage formats. This feature enables the library to work with non-standard sequence types, such as chemically modified amino acids, custom nucleotides, or integer-based sequence representations.</p>"},{"location":"design/custom_alphabets/#current-implementation","title":"Current Implementation","text":"<p>The current implementation in fast-seqfunc handles alphabets in a straightforward manner:</p> <ol> <li>Alphabets are represented as instances of the <code>Alphabet</code> class that encapsulate tokens and tokenization rules.</li> <li>Sequences can be encoded using various tokenization strategies (character-based, delimited, or regex-based).</li> <li>The <code>OneHotEmbedder</code> uses alphabets to transform sequences into one-hot encodings for model training.</li> <li>Pre-defined alphabets are available for common sequence types (protein, DNA, RNA).</li> <li>Custom alphabets are supported through the <code>Alphabet</code> class.</li> <li>Sequences of different lengths can be padded to the maximum length with a configurable gap character.</li> </ol>"},{"location":"design/custom_alphabets/#alphabet-class","title":"Alphabet Class","text":"<p>The <code>Alphabet</code> class is at the core of the custom alphabets implementation:</p> <pre><code>class Alphabet:\n    \"\"\"Represent a custom alphabet for sequence encoding.\n\n    This class handles tokenization and mapping between tokens and indices,\n    supporting both single character and multi-character tokens.\n\n    :param tokens: Collection of tokens that define the alphabet\n    :param delimiter: Optional delimiter used when tokenizing sequences\n    :param name: Optional name for this alphabet\n    :param description: Optional description\n    :param gap_character: Character to use for padding sequences (default: \"-\")\n    \"\"\"\n\n    def __init__(\n        self,\n        tokens: Iterable[str],\n        delimiter: Optional[str] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        gap_character: str = \"-\",\n    )\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Get the number of unique tokens in the alphabet.\"\"\"\n\n    def tokenize(self, sequence: str) -&gt; List[str]:\n        \"\"\"Convert a sequence string to tokens.\n\n        :param sequence: The input sequence\n        :return: List of tokens\n        \"\"\"\n\n    def pad_sequence(self, sequence: str, length: int) -&gt; str:\n        \"\"\"Pad a sequence to the specified length.\n\n        :param sequence: The sequence to pad\n        :param length: Target length\n        :return: Padded sequence\n        \"\"\"\n\n    def tokens_to_sequence(self, tokens: List[str]) -&gt; str:\n        \"\"\"Convert tokens back to a sequence string.\n\n        :param tokens: List of tokens\n        :return: Sequence string\n        \"\"\"\n\n    def indices_to_sequence(\n        self, indices: Sequence[int], delimiter: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"Convert a list of token indices back to a sequence string.\n\n        :param indices: List of token indices\n        :param delimiter: Optional delimiter to use (overrides the alphabet's default)\n        :return: Sequence string\n        \"\"\"\n\n    def encode_to_indices(self, sequence: str) -&gt; List[int]:\n        \"\"\"Convert a sequence string to token indices.\n\n        :param sequence: The input sequence\n        :return: List of token indices\n        \"\"\"\n\n    def decode_from_indices(\n        self, indices: Sequence[int], delimiter: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"Decode token indices back to a sequence string.\n\n        This is an alias for indices_to_sequence.\n\n        :param indices: List of token indices\n        :param delimiter: Optional delimiter to use\n        :return: Sequence string\n        \"\"\"\n\n    def validate_sequence(self, sequence: str) -&gt; bool:\n        \"\"\"Check if a sequence can be fully tokenized with this alphabet.\n\n        :param sequence: The sequence to validate\n        :return: True if sequence is valid, False otherwise\n        \"\"\"\n\n    @classmethod\n    def from_config(cls, config: Dict) -&gt; \"Alphabet\":\n        \"\"\"Create an Alphabet instance from a configuration dictionary.\n\n        :param config: Dictionary with alphabet configuration\n        :return: Alphabet instance\n        \"\"\"\n\n    @classmethod\n    def from_json(cls, path: Union[str, Path]) -&gt; \"Alphabet\":\n        \"\"\"Load an alphabet from a JSON file.\n\n        :param path: Path to the JSON configuration file\n        :return: Alphabet instance\n        \"\"\"\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Convert the alphabet to a dictionary for serialization.\n\n        :return: Dictionary representation\n        \"\"\"\n\n    def to_json(self, path: Union[str, Path]) -&gt; None:\n        \"\"\"Save the alphabet to a JSON file.\n\n        :param path: Path to save the configuration\n        \"\"\"\n\n    @classmethod\n    def protein(cls, gap_character: str = \"-\") -&gt; \"Alphabet\":\n        \"\"\"Create a standard protein alphabet.\n\n        :param gap_character: Character to use for padding (default: \"-\")\n        :return: Alphabet for standard amino acids\n        \"\"\"\n\n    @classmethod\n    def dna(cls, gap_character: str = \"-\") -&gt; \"Alphabet\":\n        \"\"\"Create a standard DNA alphabet.\n\n        :param gap_character: Character to use for padding (default: \"-\")\n        :return: Alphabet for DNA\n        \"\"\"\n\n    @classmethod\n    def rna(cls, gap_character: str = \"-\") -&gt; \"Alphabet\":\n        \"\"\"Create a standard RNA alphabet.\n\n        :param gap_character: Character to use for padding (default: \"-\")\n        :return: Alphabet for RNA\n        \"\"\"\n\n    @classmethod\n    def integer(\n        cls, max_value: int, gap_value: str = \"-1\", gap_character: str = \"-\"\n    ) -&gt; \"Alphabet\":\n        \"\"\"Create an integer-based alphabet (0 to max_value).\n\n        :param max_value: Maximum integer value (inclusive)\n        :param gap_value: String representation of the gap value (default: \"-1\")\n        :param gap_character: Character to use for padding in string representation\n            (default: \"-\")\n        :return: Alphabet with integer tokens\n        \"\"\"\n</code></pre>"},{"location":"design/custom_alphabets/#onehotembedder-implementation","title":"OneHotEmbedder Implementation","text":"<p>The <code>OneHotEmbedder</code> class works with the <code>Alphabet</code> class to create one-hot encodings:</p> <pre><code>class OneHotEmbedder:\n    \"\"\"One-hot encoding for protein or nucleotide sequences.\n\n    :param sequence_type: Type of sequences to encode (\"protein\", \"dna\", \"rna\",\n        or \"auto\")\n    :param alphabet: Custom alphabet to use for encoding (overrides sequence_type)\n    :param max_length: Maximum sequence length (will pad/truncate to this length)\n    :param pad_sequences: Whether to pad sequences of different lengths\n        to the maximum length\n    :param gap_character: Character to use for padding (default: \"-\")\n    \"\"\"\n\n    def __init__(\n        self,\n        sequence_type: Literal[\"protein\", \"dna\", \"rna\", \"auto\"] = \"auto\",\n        alphabet: Optional[Alphabet] = None,\n        max_length: Optional[int] = None,\n        pad_sequences: bool = True,\n        gap_character: str = \"-\",\n    )\n\n    @property\n    def alphabet(self):\n        \"\"\"Get the alphabet, supporting both old and new API.\"\"\"\n\n    def fit(self, sequences: Union[List[str], pd.Series]) -&gt; \"OneHotEmbedder\":\n        \"\"\"Determine alphabet and set up the embedder.\n\n        :param sequences: Sequences to fit to\n        :return: Self for chaining\n        \"\"\"\n\n    def transform(\n        self, sequences: Union[List[str], pd.Series]\n    ) -&gt; Union[np.ndarray, List[np.ndarray]]:\n        \"\"\"Transform sequences to one-hot encodings.\n\n        If sequences are of different lengths and pad_sequences=True, they\n        will be padded to the max_length with the gap character.\n\n        If pad_sequences=False, this returns a list of arrays of different sizes.\n\n        :param sequences: List or Series of sequences to embed\n        :return: Array of one-hot encodings if pad_sequences=True,\n            otherwise list of arrays\n        \"\"\"\n\n    def fit_transform(\n        self, sequences: Union[List[str], pd.Series]\n    ) -&gt; Union[np.ndarray, List[np.ndarray]]:\n        \"\"\"Fit and transform in one step.\n\n        :param sequences: Sequences to encode\n        :return: Array of one-hot encodings if pad_sequences=True,\n            otherwise list of arrays\n        \"\"\"\n</code></pre>"},{"location":"design/custom_alphabets/#helper-functions","title":"Helper Functions","text":""},{"location":"design/custom_alphabets/#get_embedder","title":"get_embedder","text":"<pre><code>def get_embedder(method: str, **kwargs) -&gt; OneHotEmbedder:\n    \"\"\"Get an embedder instance based on method name.\n\n    Currently only supports one-hot encoding.\n\n    :param method: Embedding method (only \"one-hot\" supported)\n    :param kwargs: Additional arguments to pass to the embedder\n    :return: Configured embedder\n    \"\"\"\n</code></pre>"},{"location":"design/custom_alphabets/#infer_alphabet","title":"infer_alphabet","text":"<pre><code>def infer_alphabet(\n    sequences: List[str], delimiter: Optional[str] = None, gap_character: str = \"-\"\n) -&gt; Alphabet:\n    \"\"\"Infer an alphabet from a list of sequences.\n\n    :param sequences: List of sequences to analyze\n    :param delimiter: Optional delimiter used in sequences\n    :param gap_character: Character to use for padding\n    :return: Inferred Alphabet\n    \"\"\"\n</code></pre>"},{"location":"design/custom_alphabets/#usage-examples","title":"Usage Examples","text":""},{"location":"design/custom_alphabets/#creating-custom-alphabets","title":"Creating Custom Alphabets","text":"<pre><code># Standard alphabets\nprotein_alphabet = Alphabet.protein()\ndna_alphabet = Alphabet.dna()\nrna_alphabet = Alphabet.rna()\n\n# Custom alphabet with standard and modified amino acids\naa_tokens = list(\"ACDEFGHIKLMNPQRSTVWY\") + [\"pS\", \"pT\", \"pY\", \"me3K\"]\nmod_aa_alphabet = Alphabet(\n    tokens=aa_tokens,\n    name=\"modified_aa\",\n    gap_character=\"X\"\n)\n\n# Integer alphabet (0-29 with -1 as gap value)\nint_alphabet = Alphabet.integer(max_value=29, gap_value=\"-1\")\n\n# Custom alphabet from configuration\nalphabet = Alphabet.from_json(\"path/to/alphabet_config.json\")\n</code></pre>"},{"location":"design/custom_alphabets/#using-the-onehotembedder","title":"Using the OneHotEmbedder","text":"<pre><code># Auto-detect sequence type\nembedder = get_embedder(\"one-hot\")\nembeddings = embedder.fit_transform(sequences)\n\n# Specify sequence type\nembedder = get_embedder(\"one-hot\", sequence_type=\"protein\", pad_sequences=True)\nembeddings = embedder.fit_transform(sequences)\n\n# Use custom alphabet\nembedder = get_embedder(\"one-hot\", alphabet=mod_aa_alphabet)\nembeddings = embedder.fit_transform(sequences)\n\n# Control padding behavior\nembedder = get_embedder(\"one-hot\", max_length=10, pad_sequences=True, gap_character=\"X\")\nembeddings = embedder.fit_transform(sequences)\n</code></pre>"},{"location":"design/custom_alphabets/#working-with-sequences-of-different-lengths","title":"Working with Sequences of Different Lengths","text":"<pre><code># Sequences of different lengths\nsequences = [\"ACDE\", \"KLMNPQR\", \"ST\"]\nembedder = OneHotEmbedder(sequence_type=\"protein\", pad_sequences=True)\nembeddings = embedder.fit_transform(sequences)\n# Sequences are padded to length 7: \"ACDE---\", \"KLMNPQR\", \"ST-----\"\n\n# Disable padding (returns a list of arrays of different sizes)\nembedder = OneHotEmbedder(sequence_type=\"protein\", pad_sequences=False)\nembedding_list = embedder.fit_transform(sequences)\n</code></pre>"},{"location":"design/custom_alphabets/#handling-special-sequence-types","title":"Handling Special Sequence Types","text":""},{"location":"design/custom_alphabets/#chemically-modified-amino-acids","title":"Chemically Modified Amino Acids","text":"<pre><code># Amino acids with modifications\naa_tokens = list(\"ACDEFGHIKLMNPQRSTVWY\") + [\"pS\", \"pT\", \"pY\", \"me3K\", \"X\"]\nmod_aa_alphabet = Alphabet(\n    tokens=aa_tokens,\n    name=\"modified_aa\",\n    gap_character=\"X\"\n)\n\n# Example sequences with modified AAs\nsequences = [\"ACDEpS\", \"KLMme3KNP\", \"QR\"]\nembedder = OneHotEmbedder(alphabet=mod_aa_alphabet, pad_sequences=True)\nembeddings = embedder.fit_transform(sequences)\n</code></pre>"},{"location":"design/custom_alphabets/#integer-based-sequences","title":"Integer-Based Sequences","text":"<pre><code># Integer representation with comma delimiter\nint_alphabet = Alphabet.integer(max_value=29, gap_value=\"-1\")\n\n# Example sequences as comma-separated integers\nsequences = [\"0,1,2\", \"10,11,12,25,14\", \"15,16\"]\nembedder = OneHotEmbedder(alphabet=int_alphabet, pad_sequences=True)\nembeddings = embedder.fit_transform(sequences)\n</code></pre>"},{"location":"design/custom_alphabets/#integration-with-model-training","title":"Integration with Model Training","text":"<pre><code># Create a custom alphabet\nalphabet = Alphabet.integer(max_value=10)\n\n# Get the embedder with the custom alphabet\nembedder = get_embedder(\"one-hot\", alphabet=alphabet)\n\n# Embed sequences\nX_train_embedded = embedder.fit_transform(train_df[sequence_col])\n\n# Create column names for the embedded features\nembed_cols = [f\"embed_{i}\" for i in range(X_train_embedded.shape[1])]\n\n# Create DataFrame for model training\ntrain_processed = pd.DataFrame(X_train_embedded, columns=embed_cols)\ntrain_processed[\"target\"] = train_df[target_col].values\n\n# Now train your model with train_processed...\n</code></pre>"},{"location":"design/custom_alphabets/#alphabet-configuration-format","title":"Alphabet Configuration Format","text":"<p>You can save and load alphabet configurations using JSON files:</p> <pre><code>{\n  \"name\": \"modified_amino_acids\",\n  \"description\": \"Amino acids with chemical modifications\",\n  \"tokens\": [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\", \"pS\", \"pT\", \"pY\", \"me3K\", \"-\"],\n  \"delimiter\": null,\n  \"gap_character\": \"-\"\n}\n</code></pre> <p>For integer-based representations:</p> <pre><code>{\n  \"name\": \"amino_acid_indices\",\n  \"description\": \"Numbered amino acids (0-25) with comma delimiter\",\n  \"tokens\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"-1\"],\n  \"delimiter\": \",\",\n  \"gap_character\": \"-\"\n}\n</code></pre>"},{"location":"design/custom_alphabets/#key-features","title":"Key Features","text":"<ul> <li>Flexible Tokenization: Support for single-character, multi-character, and delimited tokens</li> <li>Custom Alphabets: Define your own token sets for any sequence type</li> <li>Gap Handling: Configurable padding for sequences of different lengths</li> <li>Standard Bioinformatics Alphabets: Built-in support for protein, DNA, and RNA</li> <li>Integer Sequences: Special support for integer-based sequence representations</li> <li>Serialization: Save and load alphabet configurations as JSON</li> <li>Automatic Type Detection: Automatically infer sequence type from content</li> </ul>"},{"location":"design/custom_alphabets/#conclusion","title":"Conclusion","text":"<p>The custom alphabets implementation in <code>fast-seqfunc</code> provides a flexible, robust solution for handling various sequence types and tokenization schemes. This design enables working with non-standard sequence types, mixed-length characters, and integer-based sequences in a clean, consistent way.</p>"},{"location":"design/overall/","title":"Fast-SeqFunc: Design Document","text":""},{"location":"design/overall/#overview","title":"Overview","text":"<p>Fast-SeqFunc is a Python package designed for efficient sequence-function modeling of proteins and nucleotide sequences. It provides a simple, high-level API that handles sequence embedding methods and automates model selection and training through the PyCaret framework.</p> <p>The primary purpose of Fast-SeqFunc is to quickly detect whether there is meaningful \"signal\" in sequence-function data. By enabling rapid model development, researchers can determine early if predictive relationships exist, opportunistically use these models for ranking candidate sequences, and make informed decisions about investing in more complex modeling approaches when signal is detected.</p>"},{"location":"design/overall/#design-goals","title":"Design Goals","text":"<ol> <li>Simplicity: Provide a clean, intuitive API for training sequence-function models</li> <li>Flexibility: Support multiple sequence types with custom alphabet capabilities</li> <li>Automation: Leverage PyCaret to automate model selection and hyperparameter tuning</li> <li>Performance: Enable efficient processing through lazy loading and clean architecture</li> <li>Signal Detection: Rapidly determine if predictive relationships exist in the data</li> <li>Decision Support: Help users make informed choices about modeling approaches based on signal strength</li> <li>Candidate Ranking: Enable efficient prioritization of sequences for experimental testing</li> </ol>"},{"location":"design/overall/#architecture","title":"Architecture","text":""},{"location":"design/overall/#core-components","title":"Core Components","text":"<p>The package is structured around these key components:</p> <ol> <li>Core API (<code>core.py</code>)</li> <li>High-level functions for training, prediction, and model management</li> <li> <p>Handles data loading and orchestration between embedders and models</p> </li> <li> <p>Embedders (<code>embedders.py</code>)</p> </li> <li><code>OneHotEmbedder</code>: One-hot encoding for protein, DNA, RNA, and custom alphabets</li> <li> <p>Factory function <code>get_embedder</code> to create embedder instances</p> </li> <li> <p>Alphabets (<code>alphabets.py</code>)</p> </li> <li><code>Alphabet</code> class for representing character sets and tokenization rules</li> <li>Support for standard alphabets (protein, DNA, RNA) and custom alphabets</li> <li> <p>Handles mixed-length tokens and various sequence formats</p> </li> <li> <p>Models (<code>models.py</code>)</p> </li> <li><code>SequenceFunctionModel</code>: Main model class integrating with PyCaret</li> <li> <p>Handles training, prediction, evaluation, and persistence</p> </li> <li> <p>CLI (<code>cli.py</code>)</p> </li> <li>Command-line interface built with Typer</li> <li> <p>Commands for training, prediction, and embedding comparison</p> </li> <li> <p>Synthetic Data (<code>synthetic.py</code>)</p> </li> <li>Functions for generating synthetic sequence-function data</li> <li>Various task generators for different use cases</li> </ol>"},{"location":"design/overall/#data-flow","title":"Data Flow","text":"<ol> <li>User provides sequence-function data (sequences + target values)</li> <li>Data is validated and preprocessed</li> <li>Sequences are embedded using the selected embedding method</li> <li>PyCaret explores various ML models on the embeddings</li> <li>Best model is selected, fine-tuned, and returned</li> <li>Results and model artifacts are saved</li> </ol>"},{"location":"design/overall/#api-design","title":"API Design","text":""},{"location":"design/overall/#high-level-api","title":"High-Level API","text":"<pre><code>from fast_seqfunc import train_model, predict, load_model\n\n# Train a model\nmodel_info = train_model(\n    train_data=\"train_data.csv\",\n    test_data=\"test_data.csv\",\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"regression\",\n    optimization_metric=\"r2\",\n)\n\n# Make predictions\npredictions = predict(model_info, new_sequences)\n\n# Save and load models\nwith open(\"model.pkl\", \"wb\") as f:\n    pickle.dump(model_info, f)\n\nwith open(\"model.pkl\", \"rb\") as f:\n    loaded_model = pickle.load(f)\n</code></pre>"},{"location":"design/overall/#command-line-interface","title":"Command-Line Interface","text":"<p>The CLI provides commands for training, prediction, and embedding comparison:</p> <pre><code># Train a model\nfast-seqfunc train train_data.csv --sequence-col sequence --target-col function --embedding-method one-hot\n\n# Make predictions\nfast-seqfunc predict model.pkl new_sequences.csv --output-path predictions.csv\n\n# Compare embedding methods\nfast-seqfunc compare-embeddings train_data.csv --test-data test_data.csv\n</code></pre>"},{"location":"design/overall/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"design/overall/#1-embedding-strategy","title":"1. Embedding Strategy","text":"<ul> <li>One-Hot Encoding: Primary embedding method for all sequence types</li> <li>Custom Alphabets: Support for user-defined alphabets through the <code>Alphabet</code> class</li> <li>Auto-Detection: Auto-detection of sequence type (protein, DNA, RNA)</li> <li>Gap Handling: Configurable padding for sequences of different lengths</li> </ul>"},{"location":"design/overall/#2-alphabet-design","title":"2. Alphabet Design","text":"<ul> <li>Flexible Tokenization: Support for character-based, delimited, and regex-based tokenization</li> <li>Standard Alphabets: Built-in support for protein, DNA, and RNA</li> <li>Token Mapping: Bidirectional mapping between tokens and indices</li> <li>Sequence Padding: Automatic handling of variable-length sequences</li> </ul>"},{"location":"design/overall/#3-model-integration","title":"3. Model Integration","text":"<ul> <li>PyCaret Integration: Leverage PyCaret for automated model selection</li> <li>Model Type Flexibility: Support for regression and classification tasks</li> <li>Performance Evaluation: Built-in metrics calculation based on model type</li> <li>Serialization: Simple model saving and loading</li> </ul>"},{"location":"design/overall/#4-synthetic-data-generation","title":"4. Synthetic Data Generation","text":"<ul> <li>Task Generators: Functions for creating various sequence-function relationships</li> <li>Customization: Configurable difficulty, noise, and relationship types</li> <li>Data Types: Support for protein, DNA, RNA, and integer sequences</li> </ul>"},{"location":"design/overall/#implementation-details","title":"Implementation Details","text":""},{"location":"design/overall/#onehotembedder","title":"OneHotEmbedder","text":"<ul> <li>Supports protein, DNA, RNA, and custom sequences</li> <li>Auto-detects sequence type when configured to 'auto'</li> <li>Handles padding and truncating with configurable gap character</li> <li>Provides both flattened and 2D one-hot encodings</li> </ul>"},{"location":"design/overall/#alphabet-class","title":"Alphabet Class","text":"<ul> <li>Represents sets of tokens with various tokenization strategies</li> <li>Provides factory methods for standard biological alphabets</li> <li>Supports custom token sets with arbitrary delimiters</li> <li>Handles sequence padding and token-to-index mappings</li> </ul>"},{"location":"design/overall/#sequencefunctionmodel","title":"SequenceFunctionModel","text":"<ul> <li>Integrates with PyCaret for model training</li> <li>Handles different model types (regression, classification)</li> <li>Provides model evaluation methods</li> <li>Supports serialization for saving/loading</li> </ul>"},{"location":"design/overall/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<ul> <li>Generate random sequences with controlled properties</li> <li>Create sequence-function datasets with known relationships</li> <li>Support for various task types (count, position, pattern, etc.)</li> <li>Configurable noise and complexity levels</li> </ul>"},{"location":"design/overall/#dependencies","title":"Dependencies","text":"<ul> <li>Core dependencies:</li> <li>pandas: Data handling</li> <li>numpy: Numerical operations</li> <li>pycaret: Automated ML</li> <li>scikit-learn: Model evaluation metrics</li> <li>loguru: Logging</li> <li>typer: CLI</li> <li>lazy-loader: Lazy imports</li> </ul>"},{"location":"design/overall/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Advanced Embedders:</li> <li>Implement CARP integration for protein embeddings</li> <li> <p>Implement ESM2 integration for protein embeddings</p> </li> <li> <p>Caching Mechanism:</p> </li> <li> <p>Add disk caching for embeddings to improve performance on repeated runs</p> </li> <li> <p>Enhance PyCaret Integration:</p> </li> <li>Add more customization options for model selection</li> <li> <p>Support for custom models</p> </li> <li> <p>Expand Data Loading:</p> </li> <li>Support for FASTA file formats</li> <li> <p>Support for more complex dataset structures</p> </li> <li> <p>Add Visualization:</p> </li> <li>Built-in visualizations for model performance</li> <li>Sequence importance analysis</li> </ol>"},{"location":"design/overall/#conclusion","title":"Conclusion","text":"<p>Fast-SeqFunc provides a streamlined approach to sequence-function modeling with a focus on simplicity and automation. The architecture balances flexibility with ease of use, allowing users to train models with minimal code while providing options for custom alphabets and sequence types.</p> <p>The current implementation focuses on one-hot encoding with strong support for custom alphabets, while laying the groundwork for more advanced embedding methods in the future.</p>"},{"location":"releases/v0.0.3/","title":"V0.0.3","text":""},{"location":"releases/v0.0.3/#version-003","title":"Version 0.0.3","text":"<p>This release focuses on updating dependencies and versioning configurations to ensure compatibility and maintainability.</p>"},{"location":"releases/v0.0.3/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this release.</li> </ul>"},{"location":"releases/v0.0.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes were included in this release.</li> </ul>"},{"location":"releases/v0.0.3/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were introduced in this release.</li> </ul>"},{"location":"releases/v0.0.3/#other-updates","title":"Other Updates","text":"<ul> <li>Updated the version of fast-seqfunc to 0.0.2 in project files, including <code>pyproject.toml</code> and <code>pixi.lock</code>, and adjusted the SHA256 hash accordingly. (afff66) (Eric Ma)</li> <li>Updated the version in the bumpversion configuration file to 0.0.2, ensuring that the commit and tag options remain enabled for versioning. (6536e2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.1.0/","title":"V0.1.0","text":""},{"location":"releases/v0.1.0/#version-010","title":"Version 0.1.0","text":"<p>This release introduces significant enhancements to the synthetic data generation capabilities, along with improvements in code clarity and CI/CD processes.</p>"},{"location":"releases/v0.1.0/#new-features","title":"New Features","text":"<ul> <li>Added support for generating synthetic data tasks involving integer sequences. This includes new task generators for operations such as sum, max, and pattern-based tasks. The existing task generators have been enhanced to accommodate both biological and integer sequence types. The command-line interface (CLI) and documentation have been updated to reflect these new options. (555f0c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.1.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the continuous integration (CI) configuration to prevent the publishing of PyPI packages during pull request events, ensuring that packages are only published when appropriate. (7b9d65) (Eric Ma)</li> </ul>"},{"location":"releases/v0.1.0/#refactoring","title":"Refactoring","text":"<ul> <li>Refactored variable names in <code>synthetic.py</code> for improved clarity and consistency. Corresponding test cases in <code>test_synthetic.py</code> were adjusted to align with these changes. Comments were added to clarify default values. (29d56d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.1.1/","title":"V0.1.1","text":""},{"location":"releases/v0.1.1/#version-011","title":"Version 0.1.1","text":"<p>This release introduces a new feature to the public API and enhances the documentation for better clarity and usability.</p>"},{"location":"releases/v0.1.1/#new-features","title":"New Features","text":"<ul> <li>Added the <code>save_model</code> function to the public API, allowing users to easily save their models. This function is now part of the <code>fast_seqfunc</code> module's public interface. (fdfe8e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.1.1/#documentation-enhancements","title":"Documentation Enhancements","text":"<ul> <li>Enhanced the documentation with detailed explanations on signal detection and decision-making processes. This includes new sections on the purpose and methodology of signal detection in sequence-function data, along with examples and guidelines for interpreting results. The roadmap has also been updated with plans for advanced signal analysis tools and visualization enhancements. (65e54d) (Eric Ma)</li> <li>Updated the README to reflect current implementation details and planned features, ensuring that example code aligns with the current state and removing references to unimplemented features. (506d20) (Eric Ma)</li> <li>Restructured and updated the design documentation, removing outdated documents and adding new sections for improved clarity. A new command-line interface section has been introduced in the quickstart guide. (f23b13) (Eric Ma)</li> </ul> <p>No bug fixes or deprecations were included in this release.</p>"},{"location":"tutorials/classification_tutorial/","title":"Sequence Classification with Fast-SeqFunc","text":"<p>This tutorial demonstrates how to use <code>fast-seqfunc</code> for classification problems, where you want to predict discrete categories from biological sequences.</p>"},{"location":"tutorials/classification_tutorial/#overview","title":"Overview","text":"<p>In sequence classification, we want to learn to predict discrete categories (e.g., protein function, gene families, or binding/non-binding sequences) from biological sequences. This tutorial will walk you through:</p> <ol> <li>Setting up your environment</li> <li>Preparing sequence classification data</li> <li>Training binary and multi-class classification models</li> <li>Evaluating model performance</li> <li>Making predictions on new sequences</li> <li>Visualizing classification results</li> </ol>"},{"location":"tutorials/classification_tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>The following packages:   <pre><code>pip install fast-seqfunc pandas numpy matplotlib seaborn scikit-learn loguru\n</code></pre></li> </ul>"},{"location":"tutorials/classification_tutorial/#setup","title":"Setup","text":"<p>First, let's import all necessary packages:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    roc_curve,\n    auc,\n    precision_recall_curve,\n    average_precision_score\n)\nfrom fast_seqfunc import train_model, predict, save_model, load_model\nfrom loguru import logger\n</code></pre>"},{"location":"tutorials/classification_tutorial/#working-with-classification-data","title":"Working with Classification Data","text":"<p>For classification tasks, each sequence is associated with a discrete class label. Let's create synthetic data for this tutorial:</p> <pre><code>from fast_seqfunc import generate_dataset_by_task\n\n# Generate a binary classification dataset\n# (sequences with or without specific patterns)\nbinary_data = generate_dataset_by_task(\n    task=\"classification\",\n    count=1000,  # Number of sequences to generate\n    length=30,   # Sequence length\n    noise_level=0.1,  # Add some noise to make the task more realistic\n)\n\n# Generate a multi-class classification dataset\nmulti_data = generate_dataset_by_task(\n    task=\"multiclass\",\n    count=1000,  # Number of sequences to generate\n    length=30,   # Sequence length\n    noise_level=0.1,  # Add some noise\n)\n\n# Examine the data\nprint(\"Binary Classification Dataset:\")\nprint(binary_data.head())\nprint(f\"Binary class distribution:\\n{binary_data['function'].value_counts()}\")\n\nprint(\"\\nMulti-class Classification Dataset:\")\nprint(multi_data.head())\nprint(f\"Multi-class distribution:\\n{multi_data['function'].value_counts()}\")\n</code></pre>"},{"location":"tutorials/classification_tutorial/#preparing-your-own-data","title":"Preparing Your Own Data","text":"<p>If you have your own data, it should be structured in a DataFrame with at least two columns: - A column containing the sequences (e.g., \"sequence\") - A column containing the class labels (e.g., \"class\" or \"function\")</p> <p>For example: <pre><code># Load your own data\n# data = pd.read_csv(\"your_classification_data.csv\")\n\n# If your classes are text labels, you might want to convert them to integers\n# from sklearn.preprocessing import LabelEncoder\n# label_encoder = LabelEncoder()\n# data['class_encoded'] = label_encoder.fit_transform(data['class'])\n</code></pre></p>"},{"location":"tutorials/classification_tutorial/#binary-classification-example","title":"Binary Classification Example","text":"<p>Let's start with a binary classification problem:</p> <pre><code># For this tutorial, we'll use our binary dataset\ndata = binary_data\n\n# Split into train and test sets (80/20 split)\ntrain_size = int(0.8 * len(data))\ntrain_data = data[:train_size].copy()\ntest_data = data[train_size:].copy()\n\nlogger.info(f\"Data split: {len(train_data)} train, {len(test_data)} test samples\")\nlogger.info(f\"Class distribution in training data:\\n{train_data['function'].value_counts()}\")\n\n# Create output directory for results\noutput_dir = Path(\"output\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"tutorials/classification_tutorial/#training-a-binary-classification-model","title":"Training a Binary Classification Model","text":"<p>Now we can train a classification model:</p> <pre><code># Train a classification model\nlogger.info(\"Training binary classification model...\")\nmodel_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",    # Column containing sequences\n    target_col=\"function\",      # Column containing class labels\n    embedding_method=\"one-hot\", # Method to convert sequences to numerical features\n    model_type=\"classification\", # Specify classification task\n    optimization_metric=\"auc\",  # Metric to optimize (auc, accuracy, f1, etc.)\n)\n\n# Display test results\nif model_info.get(\"test_results\"):\n    logger.info(\"Test metrics from training:\")\n    for metric, value in model_info[\"test_results\"].items():\n        logger.info(f\"  {metric}: {value:.4f}\")\n\n# Save the model for later use\nmodel_path = output_dir / \"binary_classification_model.pkl\"\nsave_model(model_info, model_path)\nlogger.info(f\"Model saved to {model_path}\")\n</code></pre>"},{"location":"tutorials/classification_tutorial/#making-predictions","title":"Making Predictions","text":"<p>With our trained model, we can now make predictions:</p> <pre><code># Generate some new data for prediction\nnew_data = generate_dataset_by_task(\n    task=\"classification\",\n    count=200,\n    length=30,\n)\n\n# Make predictions\npredictions = predict(model_info, new_data[\"sequence\"])\n\n# Create results DataFrame\nresults_df = new_data.copy()\nresults_df[\"predicted_class\"] = predictions\nresults_df.to_csv(output_dir / \"binary_classification_predictions.csv\", index=False)\n\nprint(results_df.head())\n</code></pre>"},{"location":"tutorials/classification_tutorial/#evaluating-binary-classification-performance","title":"Evaluating Binary Classification Performance","text":"<p>Let's evaluate our model more thoroughly:</p> <pre><code># Calculate classification metrics\ntrue_values = test_data[\"function\"]\npredicted_values = predict(model_info, test_data[\"sequence\"])\n\n# Print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_values, predicted_values))\n\n# Create confusion matrix\ncm = confusion_matrix(true_values, predicted_values)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[\"Class 0\", \"Class 1\"],\n            yticklabels=[\"Class 0\", \"Class 1\"])\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"True Class\")\nplt.title(\"Confusion Matrix\")\nplt.tight_layout()\nplt.savefig(output_dir / \"binary_confusion_matrix.png\", dpi=300)\n\n# Try to get probability estimates (if model supports it)\ntry:\n    # For this tutorial, we'll use a simplified approach since confidence scores are not available\n    # In a real implementation, you'd need access to the raw model's predict_proba method\n\n    logger.warning(\"ROC and PR curves require probability estimates which are not supported in this version\")\n    logger.warning(\"We're skipping these visualizations in this tutorial\")\n\n    \"\"\"\n    # Below is example code that would work if your model provides probability estimates:\n\n    # Get class probabilities (if available)\n    # y_prob = model.predict_proba(X_test)[:, 1]  # probability of positive class\n\n    # Plot ROC curve\n    # fpr, tpr, _ = roc_curve(true_values, y_prob)\n    # roc_auc = auc(fpr, tpr)\n\n    # plt.figure(figsize=(8, 6))\n    # plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    # plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    # plt.xlabel('False Positive Rate')\n    # plt.ylabel('True Positive Rate')\n    # plt.title('Receiver Operating Characteristic (ROC)')\n    # plt.legend(loc=\"lower right\")\n    # plt.savefig(output_dir / \"binary_roc_curve.png\", dpi=300)\n    \"\"\"\n\nexcept Exception as e:\n    logger.warning(f\"Could not generate ROC/PR curves: {e}\")\n    logger.warning(\"This is normal if the model doesn't support probability output\")\n</code></pre>"},{"location":"tutorials/classification_tutorial/#multi-class-classification-example","title":"Multi-Class Classification Example","text":"<p>Now let's work with a multi-class problem:</p> <pre><code># Switch to multi-class data\ndata = multi_data\n\n# Split into train and test sets (80/20 split)\ntrain_size = int(0.8 * len(data))\ntrain_data = data[:train_size].copy()\ntest_data = data[train_size:].copy()\n\nlogger.info(f\"Multi-class data split: {len(train_data)} train, {len(test_data)} test samples\")\nlogger.info(f\"Class distribution in training data:\\n{train_data['function'].value_counts()}\")\n</code></pre>"},{"location":"tutorials/classification_tutorial/#training-a-multi-class-model","title":"Training a Multi-Class Model","text":"<p>Training a multi-class model is very similar to binary classification:</p> <pre><code># Train a multi-class classification model\nlogger.info(\"Training multi-class classification model...\")\nmulti_model_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"classification\",  # Can also use \"multi-class\" explicitly\n    optimization_metric=\"f1\",     # F1 with 'weighted' average is good for imbalanced classes\n)\n\n# Display test results\nif multi_model_info.get(\"test_results\"):\n    logger.info(\"Multi-class test metrics from training:\")\n    for metric, value in multi_model_info[\"test_results\"].items():\n        logger.info(f\"  {metric}: {value:.4f}\")\n\n# Save the model\nmulti_model_path = output_dir / \"multiclass_model.pkl\"\nsave_model(multi_model_info, multi_model_path)\nlogger.info(f\"Multi-class model saved to {multi_model_path}\")\n</code></pre>"},{"location":"tutorials/classification_tutorial/#evaluating-multi-class-performance","title":"Evaluating Multi-Class Performance","text":"<p>Evaluation for multi-class problems:</p> <pre><code># Calculate multi-class metrics\nmulti_true_values = test_data[\"function\"]\nmulti_predicted_values = predict(multi_model_info, test_data[\"sequence\"])\n\n# Print classification report\nprint(\"\\nMulti-class Classification Report:\")\nprint(classification_report(multi_true_values, multi_predicted_values))\n\n# Create confusion matrix\nclass_labels = sorted(data[\"function\"].unique())\nmulti_cm = confusion_matrix(multi_true_values, multi_predicted_values)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(multi_cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=class_labels,\n            yticklabels=class_labels)\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"True Class\")\nplt.title(\"Multi-class Confusion Matrix\")\nplt.tight_layout()\nplt.savefig(output_dir / \"multiclass_confusion_matrix.png\", dpi=300)\n\n# Create a normalized confusion matrix for better visualization\n# with unbalanced classes\nmulti_cm_normalized = multi_cm.astype('float') / multi_cm.sum(axis=1)[:, np.newaxis]\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(multi_cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\",\n            xticklabels=class_labels,\n            yticklabels=class_labels)\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"True Class\")\nplt.title(\"Normalized Multi-class Confusion Matrix\")\nplt.tight_layout()\nplt.savefig(output_dir / \"multiclass_normalized_confusion_matrix.png\", dpi=300)\n</code></pre>"},{"location":"tutorials/classification_tutorial/#visualizing-sequence-features-by-class","title":"Visualizing Sequence Features by Class","text":"<p>For classification tasks, it can be useful to visualize sequence properties by class:</p> <pre><code># Calculate sequence length by class\ndata[\"seq_length\"] = data[\"sequence\"].str.len()\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=\"function\", y=\"seq_length\", data=data)\nplt.title(\"Sequence Length Distribution by Class\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Sequence Length\")\nplt.tight_layout()\nplt.savefig(output_dir / \"seq_length_by_class.png\", dpi=300)\n\n# For DNA/RNA sequences, calculate nucleotide composition by class\nif any(nuc in data[\"sequence\"].iloc[0].upper() for nuc in \"ACGT\"):\n    data[\"A_percent\"] = data[\"sequence\"].apply(lambda x: x.upper().count(\"A\") / len(x) * 100)\n    data[\"C_percent\"] = data[\"sequence\"].apply(lambda x: x.upper().count(\"C\") / len(x) * 100)\n    data[\"G_percent\"] = data[\"sequence\"].apply(lambda x: x.upper().count(\"G\") / len(x) * 100)\n    data[\"T_percent\"] = data[\"sequence\"].apply(lambda x: x.upper().count(\"T\") / len(x) * 100)\n\n    # Melt the data for easier plotting\n    plot_data = pd.melt(\n        data,\n        id_vars=[\"function\"],\n        value_vars=[\"A_percent\", \"C_percent\", \"G_percent\", \"T_percent\"],\n        var_name=\"Nucleotide\",\n        value_name=\"Percentage\"\n    )\n\n    # Plot nucleotide composition by class\n    plt.figure(figsize=(12, 8))\n    sns.boxplot(x=\"function\", y=\"Percentage\", hue=\"Nucleotide\", data=plot_data)\n    plt.title(\"Nucleotide Composition by Class\")\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Percentage (%)\")\n    plt.tight_layout()\n    plt.savefig(output_dir / \"nucleotide_composition_by_class.png\", dpi=300)\n</code></pre>"},{"location":"tutorials/classification_tutorial/#working-with-imbalanced-classes","title":"Working with Imbalanced Classes","text":"<p>When dealing with imbalanced class distributions (where one class is much more frequent than others), you can use special techniques:</p> <pre><code># Example: Create an imbalanced dataset\nfrom sklearn.utils import resample\n\n# Assume class 0 is much more frequent than class 1\nclass_0 = binary_data[binary_data['function'] == 0]\nclass_1 = binary_data[binary_data['function'] == 1]\n\n# Downsample class 0 to match class 1\nclass_0_downsampled = resample(\n    class_0,\n    replace=False,  # Don't sample with replacement\n    n_samples=len(class_1),  # Match minority class\n    random_state=42  # For reproducibility\n)\n\n# Combine the downsampled majority class with the minority class\nbalanced_data = pd.concat([class_0_downsampled, class_1])\n\n# Now you can train on this balanced dataset\nprint(f\"Original class distribution: {binary_data['function'].value_counts()}\")\nprint(f\"Balanced class distribution: {balanced_data['function'].value_counts()}\")\n</code></pre> <p>Alternatively, you can use class weights in PyCaret:</p> <pre><code># Train with class weights (handled automatically by PyCaret)\nweighted_model_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"classification\",\n    optimization_metric=\"f1\",  # F1 is good for imbalanced classes\n    # Additional PyCaret settings for imbalanced data:\n    fix_imbalance=True,  # Automatically fix class imbalance\n)\n</code></pre>"},{"location":"tutorials/classification_tutorial/#loading-and-using-a-classification-model","title":"Loading and Using a Classification Model","text":"<p>You can load a saved model and use it for predictions:</p> <pre><code># Load a previously saved classification model\nloaded_model_info = load_model(model_path)\n\n# Use the model to classify new sequences\nsequences_to_classify = [\n    \"ACGTACGTACGTACGTACGTACGTACGTAC\",\n    \"GATAGATAGATAGATAGATAGATAGATA\",\n    \"CTACCTACCTACCTACCTACCTACCTAC\"\n]\n\n# Make predictions\npredictions = predict(loaded_model_info, sequences_to_classify)\n\n# Print results\nfor seq, pred in zip(sequences_to_classify, predictions):\n    print(f\"Sequence (first 10 chars): {seq[:10]}... | Predicted class: {pred}\")\n</code></pre>"},{"location":"tutorials/classification_tutorial/#advanced-model-training-options","title":"Advanced Model Training Options","text":"<p><code>fast-seqfunc</code> uses PyCaret behind the scenes, allowing customization:</p> <pre><code># Example with more options\nadvanced_model_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"classification\",\n    optimization_metric=\"f1\",\n    # Additional PyCaret setup options:\n    n_jobs=-1,  # Use all available CPU cores\n    fold=5,     # 5-fold cross-validation\n    normalize=True,  # Normalize features\n    feature_selection=True,  # Perform feature selection\n    # Classification-specific options:\n    fix_imbalance=True,  # For imbalanced datasets\n    remove_outliers=True,  # Remove outliers\n)\n</code></pre>"},{"location":"tutorials/classification_tutorial/#conclusion","title":"Conclusion","text":"<p>You've now learned how to: 1. Prepare sequence classification data 2. Train binary and multi-class classification models 3. Evaluate classification performance 4. Make predictions on new sequences 5. Handle special cases like imbalanced classes</p> <p>For more advanced features and applications, check out the API reference and additional tutorials.</p>"},{"location":"tutorials/classification_tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Try different classification tasks (e.g., protein function prediction)</li> <li>Experiment with different model types and parameters</li> <li>Apply these techniques to your own sequence classification data</li> </ul>"},{"location":"tutorials/regression_tutorial/","title":"Sequence Regression with Fast-SeqFunc","text":"<p>This tutorial demonstrates how to use <code>fast-seqfunc</code> for regression problems, where you want to predict continuous values from biological sequences.</p>"},{"location":"tutorials/regression_tutorial/#overview","title":"Overview","text":"<p>In sequence regression, we want to learn to predict a continuous value (e.g., binding affinity, enzyme efficiency, or protein stability) from a biological sequence (DNA, RNA, or protein). This tutorial will walk you through:</p> <ol> <li>Setting up your environment</li> <li>Preparing sequence-function data</li> <li>Training a regression model</li> <li>Evaluating model performance</li> <li>Making predictions on new sequences</li> <li>Visualizing results</li> </ol>"},{"location":"tutorials/regression_tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>The following packages:   <pre><code>pip install fast-seqfunc pandas numpy matplotlib seaborn scikit-learn loguru\n</code></pre></li> </ul>"},{"location":"tutorials/regression_tutorial/#setup","title":"Setup","text":"<p>First, let's import all necessary packages:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom fast_seqfunc import train_model, predict, save_model, load_model\nfrom loguru import logger\n</code></pre>"},{"location":"tutorials/regression_tutorial/#working-with-sequence-function-data","title":"Working with Sequence-Function Data","text":"<p>Sequence-function data typically consists of biological sequences paired with measurements of a functional property. For this tutorial, we'll create synthetic data:</p> <pre><code>from fast_seqfunc import generate_dataset_by_task\n\n# Generate a GC content dataset as an example\n# (where the function is simply the GC content of DNA sequences)\ndata = generate_dataset_by_task(\n    task=\"gc_content\",\n    count=1000,  # Number of sequences to generate\n    length=50,   # Sequence length\n    noise_level=0.1,  # Add some noise to make the task more realistic\n)\n\n# Examine the data\nprint(data.head())\nprint(f\"Data shape: {data.shape}\")\nprint(f\"Target distribution: min={data['function'].min():.3f}, \"\n      f\"max={data['function'].max():.3f}, \"\n      f\"mean={data['function'].mean():.3f}\")\n</code></pre>"},{"location":"tutorials/regression_tutorial/#preparing-your-own-data","title":"Preparing Your Own Data","text":"<p>If you have your own data, it should be structured in a DataFrame with at least two columns: - A column containing the sequences (e.g., \"sequence\") - A column containing the target values (e.g., \"function\")</p> <p>For example: <pre><code># Load your own data\n# data = pd.read_csv(\"your_sequence_function_data.csv\")\n</code></pre></p>"},{"location":"tutorials/regression_tutorial/#splitting-data-for-training-and-testing","title":"Splitting Data for Training and Testing","text":"<p>It's important to evaluate your model on data it hasn't seen during training:</p> <pre><code># Split into train and test sets (80/20 split)\ntrain_size = int(0.8 * len(data))\ntrain_data = data[:train_size].copy()\ntest_data = data[train_size:].copy()\n\nlogger.info(f\"Data split: {len(train_data)} train, {len(test_data)} test samples\")\n\n# Create output directory for results\noutput_dir = Path(\"output\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"tutorials/regression_tutorial/#training-a-regression-model","title":"Training a Regression Model","text":"<p>Now we can train a regression model using <code>fast-seqfunc</code>:</p> <pre><code># Train a regression model\nlogger.info(\"Training regression model...\")\nmodel_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",  # Column containing sequences\n    target_col=\"function\",    # Column containing target values\n    embedding_method=\"one-hot\",  # Method to convert sequences to numerical features\n    model_type=\"regression\",     # Specify regression task\n    optimization_metric=\"r2\",    # Metric to optimize (r2, rmse, mae)\n)\n\n# Display test results\nif model_info.get(\"test_results\"):\n    logger.info(\"Test metrics from training:\")\n    for metric, value in model_info[\"test_results\"].items():\n        logger.info(f\"  {metric}: {value:.4f}\")\n\n# Save the model for later use\nmodel_path = output_dir / \"regression_model.pkl\"\nsave_model(model_info, model_path)\nlogger.info(f\"Model saved to {model_path}\")\n</code></pre>"},{"location":"tutorials/regression_tutorial/#understanding-embedding-methods","title":"Understanding Embedding Methods","text":"<p>The <code>embedding_method</code> parameter determines how sequences are converted to numerical features:</p> <ul> <li><code>\"one-hot\"</code>: Each position in the sequence is encoded as a one-hot vector indicating which amino acid or nucleotide is present.</li> </ul> <p>Future versions of <code>fast-seqfunc</code> will include more advanced embedding methods such as ESM2 for proteins and CARP for nucleic acids.</p>"},{"location":"tutorials/regression_tutorial/#making-predictions","title":"Making Predictions","text":"<p>After training, you can use your model to make predictions on new sequences:</p> <pre><code># Generate some new data for prediction\nnew_data = generate_dataset_by_task(\n    task=\"gc_content\",\n    count=200,\n    length=50,\n)\n\n# Make predictions\npredictions = predict(model_info, new_data[\"sequence\"])\n\n# Create results DataFrame\nresults_df = new_data.copy()\nresults_df[\"predicted\"] = predictions\nresults_df.to_csv(output_dir / \"regression_predictions.csv\", index=False)\n\nprint(results_df.head())\n</code></pre>"},{"location":"tutorials/regression_tutorial/#evaluating-regression-performance","title":"Evaluating Regression Performance","text":"<p>Let's evaluate our model more thoroughly:</p> <pre><code># Calculate regression metrics\ntrue_values = test_data[\"function\"]\npredicted_values = predict(model_info, test_data[\"sequence\"])\n\n# Calculate metrics\nmse = mean_squared_error(true_values, predicted_values)\nrmse = np.sqrt(mse)\nr2 = r2_score(true_values, predicted_values)\nmae = mean_absolute_error(true_values, predicted_values)\n\n# Print metrics\nprint(f\"Test Set Performance:\")\nprint(f\"  MSE:  {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  R\u00b2:   {r2:.4f}\")\nprint(f\"  MAE:  {mae:.4f}\")\n</code></pre>"},{"location":"tutorials/regression_tutorial/#interpreting-signal-and-making-decisions","title":"Interpreting Signal and Making Decisions","text":"<p>A critical purpose of Fast-SeqFunc is to determine if there's meaningful signal in your sequence-function data. Let's look at how to interpret these results and make decisions about next steps:</p> <pre><code># Define a threshold for \"meaningful signal\"\nr2_threshold = 0.1  # This is a modest threshold for biological data\nsignal_detected = r2 &gt; r2_threshold\n\nprint(f\"Signal detected in data: {signal_detected}\")\nif signal_detected:\n    print(\"Recommendations:\")\n    print(\"  1. Use model for candidate ranking in next experiments\")\n    print(\"  2. Consider exploring more advanced embedding methods\")\n    print(f\"  3. Signal strength (R\u00b2={r2:.4f}) suggests {'strong' if r2 &gt; 0.3 else 'moderate' if r2 &gt; 0.1 else 'weak'} relationship\")\nelse:\n    print(\"Recommendations:\")\n    print(\"  1. Review experiment design and data quality\")\n    print(\"  2. Consider adding more data or different sequence features\")\n    print(\"  3. Explore if other modeling approaches detect signal\")\n</code></pre>"},{"location":"tutorials/regression_tutorial/#using-models-for-candidate-ranking","title":"Using Models for Candidate Ranking","text":"<p>If you detected signal, you can use your model to rank new candidate sequences:</p> <pre><code># Generate or import candidate sequences\nimport random\nfrom fast_seqfunc.synthetic import generate_random_sequences\n\n# Generate 1000 random candidates\ncandidate_sequences = generate_random_sequences(\n    1000, length=50, alphabet_type=\"dna\"\n)\n\n# Predict their values\ncandidate_predictions = predict(model_info, candidate_sequences)\n\n# Create DataFrame for ranking\ncandidates_df = pd.DataFrame({\n    \"sequence\": candidate_sequences,\n    \"predicted_value\": candidate_predictions\n})\n\n# Sort by predicted value (ascending or descending based on your goal)\ncandidates_df.sort_values(\"predicted_value\", ascending=False, inplace=True)\n\n# Select top candidates for experimental testing\ntop_candidates = candidates_df.head(10)\nprint(\"Top 10 candidates for experimental testing:\")\nprint(top_candidates)\n\n# Save ranked candidates\ncandidates_df.to_csv(output_dir / \"ranked_candidates.csv\", index=False)\n</code></pre> <p>This approach allows you to prioritize which sequences to test next in your experiments, potentially saving significant time and resources.</p>"},{"location":"tutorials/regression_tutorial/#visualizing-results","title":"Visualizing Results","text":"<p>Visualizations help in understanding model performance:</p> <pre><code># Create a prediction vs. actual scatter plot\nplt.figure(figsize=(10, 8))\nplt.scatter(true_values, predicted_values, alpha=0.6)\nplt.plot([true_values.min(), true_values.max()],\n         [true_values.min(), true_values.max()],\n         'r--', lw=2)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Actual vs. Predicted Values\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(output_dir / \"regression_scatter_plot.png\", dpi=300)\n\n# Plot residuals\nresiduals = true_values - predicted_values\nplt.figure(figsize=(10, 8))\nplt.scatter(predicted_values, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--', lw=2)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(output_dir / \"regression_residuals.png\", dpi=300)\n\n# Plot distribution of residuals\nplt.figure(figsize=(10, 8))\nsns.histplot(residuals, kde=True)\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Residuals\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(output_dir / \"regression_residuals_distribution.png\", dpi=300)\n\nlogger.info(\"Visualizations saved to output directory\")\n</code></pre>"},{"location":"tutorials/regression_tutorial/#working-with-different-sequence-types","title":"Working with Different Sequence Types","text":"<p><code>fast-seqfunc</code> automatically detects and handles different types of biological sequences:</p> <ul> <li>DNA (containing A, C, G, T)</li> <li>RNA (containing A, C, G, U)</li> <li>Proteins (containing amino acid letters)</li> </ul> <pre><code># Example with protein sequences\nfrom fast_seqfunc import generate_random_sequences\n\n# Generate random protein sequences\nprotein_sequences = generate_random_sequences(\n    length=30,\n    count=100,\n    alphabet=\"ACDEFGHIKLMNPQRSTVWY\",  # Protein alphabet\n    fixed_length=True,\n)\n\n# Create a dummy function (e.g., number of hydrophobic residues)\nhydrophobic = \"AVILMFYW\"\nfunction_values = [\n    sum(seq.count(aa) for aa in hydrophobic) / len(seq)\n    for seq in protein_sequences\n]\n\n# Create dataset\nprotein_data = pd.DataFrame({\n    \"sequence\": protein_sequences,\n    \"function\": function_values\n})\n\n# Now you could train a model on this protein data using the same workflow\n</code></pre>"},{"location":"tutorials/regression_tutorial/#advanced-model-training-options","title":"Advanced Model Training Options","text":"<p><code>fast-seqfunc</code> uses PyCaret behind the scenes, which allows for customization:</p> <pre><code># Example with more options\nadvanced_model_info = train_model(\n    train_data=train_data,\n    test_data=test_data,\n    sequence_col=\"sequence\",\n    target_col=\"function\",\n    embedding_method=\"one-hot\",\n    model_type=\"regression\",\n    optimization_metric=\"r2\",\n    # Additional PyCaret setup options:\n    n_jobs=-1,  # Use all available CPU cores\n    fold=5,     # 5-fold cross-validation\n    normalize=True,  # Normalize features\n    polynomial_features=True,  # Generate polynomial features\n    feature_selection=True,  # Perform feature selection\n)\n</code></pre>"},{"location":"tutorials/regression_tutorial/#loading-and-reusing-models","title":"Loading and Reusing Models","text":"<p>You can load saved models for reuse:</p> <pre><code># Load a previously saved model\nloaded_model_info = load_model(model_path)\n\n# Use the loaded model for predictions\nnew_predictions = predict(loaded_model_info, new_data[\"sequence\"])\n\n# Verify the predictions match those from the original model\nnp.allclose(predictions, new_predictions)\n</code></pre>"},{"location":"tutorials/regression_tutorial/#conclusion","title":"Conclusion","text":"<p>You've now learned how to: 1. Prepare sequence-function data 2. Train a regression model using <code>fast-seqfunc</code> 3. Make predictions on new sequences 4. Evaluate model performance 5. Visualize results</p> <p>For more advanced features and applications, check out the API reference and additional tutorials.</p>"},{"location":"tutorials/regression_tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Try different regression tasks (e.g., \"motif_position\", \"interaction\")</li> <li>Experiment with different model parameters</li> <li>Apply these techniques to your own sequence-function data</li> </ul>"}]}